{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from function import *\n",
    "\n",
    "import sentencepiece as spm\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Activation, Dense, Embedding, Flatten, Dropout, Conv2D, Reshape, GlobalMaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spm_tokenizer(train_text, vocab_size, model_prefix):\n",
    "    templates = '--input={} --model_prefix={} --vocab_size={}'\n",
    "    cmd = templates.format(train_text, model_prefix, vocab_size)\n",
    "    spm.SentencePieceTrainer.train(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling(df_pickle, embedding_dim, max_len):\n",
    "    # load spm model \n",
    "    path = os.getcwd()\n",
    "    spm_path = path + '/out/cate_spm.model'\n",
    "    sp = spm.SentencePieceProcessor()\n",
    "    sp.Load(spm_path)\n",
    "    '''\n",
    "    data  sample\n",
    "    ===================================================================================\n",
    "    mall_goods_name\tmaster_tag\n",
    "    신원 자외선 칫솔살균기 SW-15A 노랑\t구강가전\n",
    "    NS홈쇼핑 삼성전자 MC32K7056CT 세라믹조리실 오븐 32L 쇼핑도 건강하게 ...\t주방가전\n",
    "    교세라 정품 TK-5154KY P6035cdn 10K 노랑\t사무가전(프린터/복합기)\n",
    "    캐슬 Avon2 에이본2 리본트위터 북쉘프스피커\t스피커\n",
    "    ===================================================================================\n",
    "    '''\n",
    "    df = pd.read_pickle(df_pickle)\n",
    "    classes = df['master_tag'].nunique()\n",
    "    # pre processing -> if you do experiment, should be use mp\n",
    "    df['mall_goods_name'] = df['mall_goods_name'].apply(lambda x:' '.join(clean_spm(sp.encode_as_pieces(x))))\n",
    "    # generate word2vec embedding layer\n",
    "    sentences = df['mall_goods_name'].drop_duplicates().apply(lambda x:x.split(' ')).to_list()\n",
    "\n",
    "    # embedding_dim = 600\n",
    "    model = Word2Vec(sentences, size = embedding_dim, window = 5, min_count = 2, workers = 8)\n",
    "\n",
    "    word_vectors = model.wv\n",
    "    vocabs = word_vectors.vocab.keys()\n",
    "    word_vectors_list = [word_vectors[v] for v in vocabs]\n",
    "    print ('Vocab Size:',len(model.wv.vocab))\n",
    "\n",
    "    filename = path + '/out/cate_w2v.txt'\n",
    "    model.wv.save_word2vec_format(filename, binary = False)\n",
    "    # load embedding layer\n",
    "    embedding_index = {}\n",
    "    f = open(os.path.join('',filename), encoding = 'utf-8')\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:])\n",
    "        embedding_index[word] = coefs\n",
    "    f.close()\n",
    "    # train model\n",
    "    X = df[['mall_goods_name']]\n",
    "    y = df['master_tag']\n",
    "\n",
    "    label_encoder = preprocessing.LabelEncoder()\n",
    "    y = label_encoder.fit_transform(y)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 2020)\n",
    "    X_train, X_test = X_train['mall_goods_name'], X_test['mall_goods_name']\n",
    "\n",
    "    y_train = to_categorical(y_train)\n",
    "    y_test = to_categorical(y_test)\n",
    "\n",
    "    # max_len = 90 -> example\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    X_train = sequence.pad_sequences(sequences,maxlen = max_len) #  padding='post'\n",
    "    sequences = tokenizer.texts_to_sequences(X_test)\n",
    "    X_test = sequence.pad_sequences(sequences, maxlen = max_len)\n",
    "\n",
    "    # embedding_dim = 600 -> example\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    num_words = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((num_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i > num_words:\n",
    "            continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print ('num_words:',num_words)\n",
    "\n",
    "    model = Sequential()\n",
    "    embedding_layer = Embedding(num_words,\n",
    "                                embedding_dim,\n",
    "                                weights = [embedding_matrix],\n",
    "                                input_length = max_len)\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Reshape((max_len, embedding_dim, 1), input_shape = (max_len, embedding_dim)))\n",
    "    model.add(Conv2D(filters = 32, kernel_size = (4, embedding_dim), strides = (2,2), padding = 'valid'))\n",
    "    model.add(GlobalMaxPooling2D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(classes, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['acc'])\n",
    "    history = model.fit(x = X_train, y = y_train, batch_size = 128, epochs = 1, verbose = 1, validation_split = 0.1)\n",
    "    # evaluate\n",
    "    acc = model.evaluate(X_test,y_test)\n",
    "    print('Loss: {:0.3f} | Accuracy: {:0.3f}'.format(acc[0],acc[1]))\n",
    "    print ('=' * 50)\n",
    "    pred = model.predict(X_test)\n",
    "    pred_bool = np.argmax(pred,1)\n",
    "    y_test_bool = np.argmax(y_test,1)\n",
    "    print(classification_report(y_test_bool, pred_bool))\n",
    "    # save labels\n",
    "    _class = label_encoder.classes_\n",
    "    _num = [x for x in range(len(_class))]\n",
    "    mapping_dct = dict(zip(_num,_class))\n",
    "    \n",
    "    # save tkn_model\n",
    "    with open(path + '/out/cate_tkn.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    # save classification_model\n",
    "    model.save(path + '/out/cate_model.h5')\n",
    "    \n",
    "    # save labels_dictionary\n",
    "    _class = label_encoder.classes_\n",
    "    _num = [x for x in range(len(_class))]\n",
    "    mapping_dct = dict(zip(_num,_class))\n",
    "    with open(path + '/out/labels.pickle', 'wb') as handle:\n",
    "        pickle.dump(mapping_dct, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cate_pred(lst, max_len):\n",
    "    pre = [' '.join(clean_spm(sp.encode_as_pieces(text))) for text in lst]\n",
    "    t = sequence.pad_sequences(tkn.texts_to_sequences(pre), maxlen = max_len)\n",
    "    P = classification_model.predict_on_batch(t)\n",
    "    pred = [np.argmax(x) for x in P]\n",
    "    prob = [np.max(x) for x in P]\n",
    "    X = pd.Series(pred).map(mapping_dct).to_list()\n",
    "    return X, prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 6302\n",
      "num_words: 7013\n",
      "27859/27859 [==============================] - 293s 11ms/step - loss: 0.6697 - acc: 0.8207 - val_loss: 0.5658 - val_acc: 0.8492\n",
      "13758/13758 [==============================] - 16s 1ms/step - loss: 0.5657 - acc: 0.8493\n",
      "Loss: 0.566 | Accuracy: 0.849\n",
      "==================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.92      0.94      8863\n",
      "           1       0.92      0.86      0.89      5727\n",
      "           2       0.77      0.67      0.72      9077\n",
      "           3       0.91      0.84      0.87      4160\n",
      "           4       0.80      0.85      0.82      4662\n",
      "           5       0.93      0.94      0.94      7783\n",
      "           6       0.99      0.96      0.97      5585\n",
      "           7       0.95      0.96      0.95      8733\n",
      "           8       0.76      0.59      0.67      7648\n",
      "           9       0.65      0.06      0.12      2259\n",
      "          10       0.98      0.95      0.96      2827\n",
      "          11       0.59      0.34      0.43       975\n",
      "          12       0.68      0.75      0.71     13140\n",
      "          13       0.76      0.69      0.72      4662\n",
      "          14       0.80      0.83      0.82     17212\n",
      "          15       0.74      0.51      0.61      3107\n",
      "          16       0.75      0.66      0.70      1539\n",
      "          17       0.91      0.89      0.90     17979\n",
      "          18       0.92      0.93      0.93     20261\n",
      "          19       0.78      0.82      0.80      3066\n",
      "          20       0.82      0.69      0.75      1337\n",
      "          21       0.72      0.67      0.70     13769\n",
      "          22       0.85      0.83      0.84      4208\n",
      "          23       0.90      0.96      0.93     22405\n",
      "          24       0.57      0.87      0.69      9125\n",
      "          25       0.76      0.62      0.68      1792\n",
      "          26       0.66      0.73      0.69      6214\n",
      "          27       0.89      0.66      0.76       602\n",
      "          28       0.73      0.70      0.71     11869\n",
      "          29       0.88      0.94      0.91     34603\n",
      "          30       0.93      0.89      0.91      6261\n",
      "          31       0.74      0.50      0.60      2881\n",
      "          32       0.96      0.95      0.96      9498\n",
      "          33       0.87      0.91      0.89     48342\n",
      "          34       0.72      0.68      0.70      7173\n",
      "          35       0.78      0.88      0.83     14997\n",
      "          36       0.88      0.87      0.88      6804\n",
      "          37       0.89      0.65      0.75       258\n",
      "          38       0.92      0.85      0.89      2902\n",
      "          39       0.92      0.84      0.88      8586\n",
      "          40       0.91      0.79      0.85     15554\n",
      "          41       0.73      0.77      0.75      2853\n",
      "          42       0.88      0.91      0.90      8278\n",
      "          43       0.94      0.94      0.94     36715\n",
      "          44       0.65      0.46      0.54      3219\n",
      "          45       0.90      0.86      0.88     10728\n",
      "\n",
      "    accuracy                           0.85    440238\n",
      "   macro avg       0.83      0.77      0.79    440238\n",
      "weighted avg       0.85      0.85      0.85    440238\n",
      "\n",
      "CPU times: user 36min 27s, sys: 20min 2s, total: 56min 30s\n",
      "Wall time: 18min 11s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['TV', '청소기'], [0.9770483, 0.9790325])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# assign path\n",
    "path = os.getcwd()\n",
    "# spm\n",
    "train_text = path + '/data/cate.txt'\n",
    "vocab_size = 30000\n",
    "model_prefix = path + '/out/cate_spm'\n",
    "make_spm_tokenizer(train_text, vocab_size, model_prefix)\n",
    "# word2vec & classifcation model\n",
    "df_pickle = path + '/data/ele.pk'\n",
    "embedding_dim = 100\n",
    "max_len = 50\n",
    "modeling(df_pickle, embedding_dim, max_len)\n",
    "# spm_load\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load(path + '/out/cate_spm.model')\n",
    "# tkn_load\n",
    "with open(path + '/out/cate_tkn.pickle', 'rb') as handle:\n",
    "    tkn = pickle.load(handle)\n",
    "# label load\n",
    "with open(path + '/out/labels.pickle', 'rb') as handle:\n",
    "    mapping_dct = pickle.load(handle)\n",
    "# model_load\n",
    "classification_model = keras.models.load_model(path + '/out/cate_model.h5')\n",
    "# prediction new test data\n",
    "lst = ['삼성전자TV 32인치TV 43인치TV 49인치TV Full HD 삼성TV 소형TV 티비',\n",
    "      '니코 싸이클론 진공청소기 NKV-79WS']\n",
    "cate_pred(lst, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(path + '/data/ele.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mall_goods_name</th>\n",
       "      <th>master_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1676131</th>\n",
       "      <td>바보사랑 [바보사랑]19년 신형 ELO 프리미엄 골드 IH 인덕션 쿠커 BR A77...</td>\n",
       "      <td>주방가전</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256097</th>\n",
       "      <td>Coms 보조 배터리 비상 충전기(8400mAh).삼성 SDI 배</td>\n",
       "      <td>핸드폰용품/태블릿용품/케이스/보조배터리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4384607</th>\n",
       "      <td>큐사운드 음향 BKM-3600 BKM3600 4옴 1950Wx2출</td>\n",
       "      <td>음향장비</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           mall_goods_name  \\\n",
       "1676131  바보사랑 [바보사랑]19년 신형 ELO 프리미엄 골드 IH 인덕션 쿠커 BR A77...   \n",
       "1256097               Coms 보조 배터리 비상 충전기(8400mAh).삼성 SDI 배   \n",
       "4384607               큐사운드 음향 BKM-3600 BKM3600 4옴 1950Wx2출   \n",
       "\n",
       "                    master_tag  \n",
       "1676131                   주방가전  \n",
       "1256097  핸드폰용품/태블릿용품/케이스/보조배터리  \n",
       "4384607                   음향장비  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['mall_goods_name','master_tag']].sample(n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
